\documentclass[a11paper]{article}

\usepackage{karnaugh-map}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{titlepage}
\usepackage{document}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{float}
\usepackage{varwidth}
\usepackage{graphicx}
% \usepackage[toc,page]{appendix}
\usepackage[dvipsnames]{xcolor}

\title{Rapport d'APP}

\class{Architecture des ordinateurs}
\classnb{GIF310}

\teacher{Marc-André Tétrault}

\author{
  \addtolength{\tabcolsep}{-0.4em}
  \begin{tabular}{rcl} % Ajouter des auteurs au besoin
      Benjamin Chausse & -- & CHAB1704 \\
      Shawn Couture    & -- & COUS1912 \\
  \end{tabular}
}

\newcommand{\todo}[1]{\begin{color}{Red}\textbf{TODO:} #1\end{color}}
\newcommand{\note}[1]{\begin{color}{Orange}\textbf{NOTE:} #1\end{color}}
\newcommand{\fixme}[1]{\begin{color}{Fuchsia}\textbf{FIXME:} #1\end{color}}
\newcommand{\question}[1]{\begin{color}{ForestGreen}\textbf{QUESTION:} #1\end{color}}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

\section{Performances d'organisations}
Une analyze des différentes organisations proposées est effectuée dans cette
section. Cette analyze donne une vision des gains de performances qu'un
changement de la méthode d'exécution des instructions amènerait.

\subsection{Analyze du code de référence}

Le code de référence contient des pseudo-instructions assembleurs que MARS
transforme en instructions standard. Premièrement, les instructions "li" sont
des pseudo-instructions. Cependant, ces dernières sont traduite en une seule
instruction et donc n'ont pas d'impacte sur le nombre d'instructions totaux
et le temps d'exécution. Malheureusement, les 4 instructions "sw" et "lw"
dans \verb|boucle_interne| sont des pseudo-instructions puisqu'ils index sur
des tableaux directement. Ceci n'est pas possible en MIPS. MARS remplace ces
pseudo-instructions par 3 différentes instructions. En premier il charge
l'addresse du tableau dans un registre, ensuite il décale l'addresse pour
qu'elle pointe sur la bonne index du tableau et finalement un vrai "lw" ou
"sw" est effectué à l'index 0 de l'addresse décalé. Ceci veut dire que la
\verb|boucle_interne| a 6 instructions supplémentaire.

\subsection{Organisation unicycle}

%\todo{documenter sous forme algébrique l'organisation unicycle}
%\todo{Calculer en cycles d'horloge l'organisation unicycle}

Le calcul du temps d'exécution en cycle d'horloge pour une organisation
unicycle est aussi simple que de compter le nombre d'instructions exécuté.
Aucune bulles ou vidanges est nécessaire.

La boucle interne a un total de $15$ instructions qui une fois traduite donne
un total de $23$. Ces 23 instructions sont exécuté $4$ fois. Au 5e appel, une
seule instruction est exécuté, soit la comparaison \verb|beq|, qui branche
dans \verb|finBoucleInterne| qui est $2$ instructions. La formule pour la
boucle interne est donc: $$ T = 4\times(23)+(2+1) $$

la boucle externe est aussi exécuté $4$ fois. Elle contient $2$ instructions
puis une exécution complète de la boucle interne. À la 5e exécution, une
seule instruction est exécuté pour sortie de la boucle et appeller
\verb|finBoucleExterne|. Cette dernière est $2$ instructions. La formule pour
la boucle externe est donc la suivante:

\begin{equation}
T = 4\times(2+4\times(23)+(2+1)) + (2+1)
\end{equation}

Reste juste le \verb|main| à inclure, qui est $2$ instructions. Le temps
d'exécution en coup d'horloge est donc:

\begin{align}
  T &= 2+ 4\times(2+4\times(23)+(2+1)) + (2+1) \\
  T &= 4\times(4\times(23)+5) + 5 \\
  T &= 4\times(97) + 5 \\
  T &= 393
\end{align}

\subsection{Organisation en pipeline avec branchement au 4e étage}

%\todo{identifier et calcuer toutes les pénalités causées par deux organisation avec branchement au 4e étage en pipeline}
%\todo{identifier et calcuer toutes les pénalités causées par deux organisation avec branchement au 2e étage}

L'analyze de l'organisation en pipeline avec branchement au 4e étage est fait
sans unité d'avancement ni de détection des aléas de données. Ceci dit,
l'ajout de bulles est nécessaire pour évité que des instructions utilisant
les même registres rentre en concurance. Si ce n'est pas fait, une
instructions pourrait additionner dans un certain registre, l'instruction
suivante l'utiliserait mais n'aurait pas la bonne valeur car l'instruction
précédente n'a pas terminé d'exécuté. Après une analyze de l'organisation,
une bulle de $2$ instructions minimum est nécessaire entre instructions
concurente. Vu que les branchements sont au 4e étage, une bulle de $3$
\verb|nop| est nécessaire après chaque appel d'instructions de format J pour
évité que les instructions suivant les branchements soit charger dans le
pipeline lorsqu'il ne devrait pas.

Finalement, il faut prendre en compte la traduction des pseudo-instructions.
En effet, ils sont traduits en instructions qui utilise des registres
concurents! l'ajout de 2 bulles de $2$ instructions par pseudo-instructions
est donc nécessaire. La première entre le chargement de l'addresse du tableau
et son déplacement, la deuxième entre le déplacement de l'addresse et
l'écriture ou la lecture à cet endroit. Cependant, les bulles ajouter avant
l'appel des pseudo-instructions ne sont donc pas nécessaire puisque le
registre n'est qu'utilisé que 2 instructions plus loin. Ceci réduit de 8
\verb|nop| le total de la boucle interne.

Après analyze du program de référence, le \verb|main| devient 4 instructions,
la boucle interne à un nouveau total de $55$ instructions, la boucle externe
en a $8$ au lieu de $2$, le main en a $4$ au lieu de $2$, la fin de boucle
externe en a $8$ au lieu de $2$ et la fin de boucle interne en a $5$ au lieu
de $2$. La nouvelle formule est donc:

\begin{align}
  T &= 4+ 4\times(7+4\times(55)+(5+1)) + (8+1) \\
  T &= 945
\end{align}

\subsection{Organisation en pipeline avec branchement au 2e étage}

Cette organisation change l'emplacement des branchements et les montent de
$2$ étages. Ils sont donc effectué plus tôt dans la chaine d'évènement des
pipelines. Ça à pour effet que $2$ des $3$ \verb|nop| nécessaire pour les
instructions de format J peuvent être enlevé. Par la suite, cette
organisation, pour notre cas d'étude, implémente une unité d'avancement. Ceci
permet aux résultats des instructions d'être disponible plus tôt et de ne pas
devoir attendre leurs mise en mémoire (pour les instructions qui ne sont pas
des branchements), éliminant les bulles entre les accès des instructions
concurentes. Cependant, les banchements sont exécuté plus haut que
l'emplacement de retour de l'unité d'avancement. Ça veut dire que la bulle
reste nécessaire lorsque les instructions concurentes sont avec des
instructions de branchements! De plus, une unité de détection des aléas est
présente. Cette dernière est plutôt complexe mais enlève complètement le
nécessaire de rajouter des \verb|nop| manuellement pour créé des bulles.
Malheureusement, on doit tout de même calculer les délais ajoutés tel que
mentionné plus haut. Le \verb|syscall| a encore besoion de ces $4$
instructions le précédent et d'une bulle de $2$ pour le registre \verb|$v0|
avant son appel. De même, les pseudo-instructions n'ont plus de bulles et on
donc le même impacte d'ajout d'instruction que l'organisation unicycle.

Ceci veut dire que, le \verb|main| à encore $4$ instructions, la boucle
interne en a maintenant $25$, la boucle externe en a maintenant $5$, la fin
de boucle externe en a encore $8$ et la fin de boucle interne en a maintenant
$3$. La formule algébrique est donc la suivante:

\begin{align}
  T &= 4+ 4\times(5+4\times(25)+(3+1)) + (8+1) \\
  T &= 449
\end{align}

\subsection{Temps d'exécution}

%\todo{calculez le temps d'exécution en vous basant sur une vitesses d'opération de 25 ns pour l'organisation unicycle}
%\todo{calculez le temps d'exécution en vous basant sur une vitesses d'opération de 10 ns pour l'organisation pipeline}

Pour notre cas d'étude, l'organisation unicycle à un temps de $25ns$ par
instructions tandis que les organisations en pipeline en prennent $10ns$. La
formule trivial suivante indique que le temps d'exécution est le total des
instructions multiplié par le temps d'un coup d'horloge.

\begin{equation}
\text{T}_{\text{sys}} = \text{instructions}\times\text{T}_{\text{CPU}}
\end{equation}

L'unicycle prend donc $9825$ nanosecondes, le pipeline avec branchement au 4e
étage prend $9450$ nanosecondes et l'organisation en pipeline avec
branchement au 2e étage prent $4490$ nanosecondes.

\subsection{Analyze des résultats}

L'organisation en pipeline avec branchement au 2e étage est $2.2\times$ plus
performant que l'organisation unicycle avec l'avantage de d'avoir aucune
différence dans les codes assembleurs. Cependant, la modification de
l'organisation pour une méthode pipeline peut être couteuse en developpement
et nécessite un changement matérielle. Pour ce qui est de l'organisation avec
branchement au 4e étage, le gain de performance n'est que de $0.03$,
complètement négligable et nécessite l'ajout manuelle de bulles dans le
langage assembleur. Les gains ne sont pas à négligé cependant puisqu'une
amélioration de $2.2\times$ atteint presque la demande d'amélioration de
$3\times$ sans nécessairement faire $3\times$ moins d'instructions tel que
demandé.


\section{Performances SIMD}

\subsection{Instructions identifiés}
\subsubsection{Charger un vecteur}
Vue que le programme doit utilisé des vecteurs, leurs chargement doit être
possible. Donc une instruction SIMD \verb|lwv| est à implémenter. Elle vient
avec le bonus d'enlever la traduction de pseudo-instructions.

\subsubsection{Multiplication}
Le code de référence fait une multiplication entre deux données de deux
vecteurs dans la boucle interne. Hors, ceci peut être fait dans dans une
instruction SIMD, ce qui enlèvera la boucle interne éventuellement. Hors,
l'instruction \verb|multv| permet de multiplié deux vecteurs et mettre le
résultat dans un autre.

\subsubsection{Somme d'un vecteur}
L'analyse du code de référence montre qu'une somme des données est effectués.
Hors, l'utilisation d'un SIMD est possible ici. Une instruction spécial appellé
\verb|sumv| permet d'additionner les $4$ valeurs d'un vecteur ensemble et
mettre le résultat dans un registre standard. Ceci permet d'immédiatement
mettre le résultat à la bonne endroit dans le vecteur de sortie.

\subsection{Nouveau temps d'exécution}
Dans le nouveau code de référence, le \verb|main| a $5$ instructions. La boucle
interne a complètement disparût à cause des instructions SIMD. La boucle
externe a $8$ instructions. La comparaison est a la toute fin ce qui réduit
d'une instruction car la boucle n'est pas exécuté $5$ fois mais juste $4$. La
fin de boucle est $2$ instructions. De plus, il n'y a plus de pseudo-instructions.

Ceci dit, la formule de calcul du nombre d'instructions est maintenant la suivante:
$$
T = 2 + 4\times(8) + 2
$$
$$
T = 36
$$

\subsection{Comparaison avec l'unicycle}
Le total d'instructions passe de $393$ à juste $36$. C'est un gain de $10$! Ce
gain démontre emplement qu'il est possible de réduire d'au minimum $3$ fois le
nombre d'instructions total pour la problématique étant donné la nature
similaire des deux programmes assembleurs.

%\todo{identifiez les instructions qui seraient à convertir en SIMD}

%\todo{calculez le nouveau temps d'exécution en cycles d'horloge, pour enfin le
%comparer avec celui en unicycle}

%\todo{dire si le gain de performance permet d'espérer d'atteindre les objectifs
%de la problématique}



\section{Performances des mémoires sur processeur unicycle}
\subsection{Pour la mémoire de données}
\subsubsection{Cas de la DRAM de la problématique}

La DRAM de base à une pénalité de $10$ coups d'horloge par accès. Chaque accès
fournis 2 mots séquentielles. De base, aucune cache de donnée est inclus.
Donc chaque accès aux données nécessite $10$ coups d'horloge. Le mode de
lecture "2-word burst" donne aucun avantage puisque les données ne sont pas
sauvegarder dans une cache. Ça veut donc dire qu'un des deux mots chargé
n'est pas utilisé et un autre accès à la DRAM est nécessaire plus tard pour
allé chercher le deuxième mot! Cependant, si des instructions SIMD sont
utilisés alors ce mode de lecture vient sauver du temps lors de chargement de
vecteurs. Charger un vecteur de $4$ prendrait uniquement $20$ coup de clock
plutôt que $40$ sans ajout d'instructions SIMD.

Le calcul du temps d'accès pour l'implémentation de mémoire sans cache est très
simple. Chaque appel de \verb|sw| et \verb|lw| causent des délais d'accès.
Les instructions immédiate en cause aucun puisque les données sont directement
dans l'instruction directe. Une fois que les données sont dans les registres,
le coup d'accès est nulle.

Le programme matricielle de référence fait $4$ appel à ce genre
d'instructions à l'intérieur de la boucle interne. La formule mathématique
des coups d'horloge supplémentaire est donc simplement:

\begin{align}
  \text{T} &= 10\times4\times(4\times4) \\
  \text{T} &= 10\times4\times(4\times4) \\
  T        &= 640
\end{align}

Si on rajoute les instructions de l'organisation unicycle, on obtient un
total de $1033$ coup d'horloge, sans compté les pénalité de mémoire
d'instructions!

\subsubsection{Cas avec une cache attaché}

Pour réduire le nombre de pénalité engendré par l'accès à la DRAM, suffit
d'ajouter une cache. Pour ce cas d'étude, il s'agit d'une cache de $256$ blocs
avec $2$ mots de $32$ bits par block, avec un mode de DRAM "write-through".
Dans les caches, les premières accès causent une pénalité de lecture. Les accès
suivantes, au mêmes données, ne causent pas de pénalité puisque le programme n'a
plus besoin d'accèder a la DRAM. Hors, les vecteurs d'entrées et de sorties sont
$4$ mots. Sachant que la cache enregistres des données contingu à l'écriture, la
lecture initiale des vecteurs causent $2$ pénalité d'accès.
Pour la matrice, $8$ pénalité ont lieu pour toute la mettre dans la cache par
groupe de $2$ mots. La cache ayant $256$ blocs, aucun remplacement de blocs
auront lieu.

Le calcul reste simple, uniquement les pénalité d'accès mentionné plus haut
ont besoin d'être calculer puisque n'importe quel autre lecture ou écriture
n'a plus besoin d'accèder à la DRAM. Voici donc la formule:

\begin{align}
  \text{T} &= 10\times(2+2+8) \\
  \text{T} &= 120
\end{align}


\subsection{Pour les instructions}
\subsubsection{Cas de la DRAM de la problématique}

Le calcul pour les délais d'accès en mémoire pour les instructions est
relativement facile pour le cas de la DRAM de la problématique. En effet,
sans cache, chaque appelle d'instruction engendre un délai d'acces! Même si
la DRAM fournis $2$ mots (donc deux instructions) par appel, l'organisation
unicycle ne peut qu'en exécuter qu'une et sans cache, elle ne peut pas
enregistrer la prochaine instructions. Donc chaque instructions vient créé $10$
coups d'horloge supplémentaire.

Le calcul est relativement simple, il s'agit de multiplier le nombre total
d'instructions calculé plus haut, à $10$ pour obtenir les coups d'horloge
supplémentaires engendré par leurs accès. On obtient donc la performance
suivante:

\begin{equation}
  \text{T}=393\times10=3930
\end{equation}

\subsubsection{Cas avec une cache attaché}

l'attachement d'une cache à la DRAM permet l'enregistrements d'instructions
pour ne pas avoir besoin d'aller les chercher avec des pénalités à chaque
fois. De plus, les instructions dans les boucles et la fin de boucle interne
vont être chargé dans la cache qu'une seule fois. Heureusement, la cache de
$256$ blocs de $2$ mots suggéré par le cas d'étude à assez d'espace pour
qu'aucun cas de remplacement de bloc ait lieu. Le calcul reste facile à
faire. À chaque deux instructions, une pénalité a lieu. Il suffit donc de
compté le nombre d'instructions unique dans le programme, divisé par $2$ et
multiplié par $10$ns et on obtient la pénalité en nanosecondes de l'accès à
la DRAM.

Hors, on a $23$ instructions uniques, incluant la traduction des
pseudo-instructions dans le programme de référence. Suffisais simplement de
compté le nombre de lignes. La formule mathématique pour le nombre de coup 
d'horloge supplémentaire est donc la suivante:

\begin{align}
  \text{T} &= 10\times\lceil23/2\rceil \\
  \text{T} &= 10\times12 \\
  \text{T} &= 120
\end{align}

\subsection{Analyze des résultats}

Après l'analyze de configurations de cache, il est évident que l'ajout d'une
mémoire cache à la DRAM donne des gains significatifs à la performance. 
En effet, un gain d'un facteur de $32$ est observé pour la mémoire
d'instructions! Un gain de $5.3$ est observé pour les accès aux données.
Il est donc évidant que l'ajout d'une cache apporte un gain significatif en
terme de performance. Cependant, ceci coute un changement au niveau physique
ce qui est couteux et ne réduit pas le nombres d'instructions exécuter. Mais
avec un gain de performance de $32$ ça pourrais réduire significativement leurs
problème.

\section{Configuration des caches}
%\todo{définir nos choix pour la configuration de la cache pour minimiser la
%taille ET les pénalités d'accès aux données en clock cycles}
Le programme de référence ainsi que l'algorithme de Vitberbi utilisent des
tableaux séparable en $4$. En effet, le meilleur implémentation de SIMD pour
les programmes est d'utiliser des instructions qui gèrent des vecteurs de $4$.
De plus, les deux algorithmes utilisent la même structures de données. Soit une
matrice $4\times4$ et deux tableaux de $1\times4$. Optimiser pour un programme
optimisera donc l'autre à un certain point.

Ceci dit, la cache s'occupe de gérer les requètes et est très versatile. Par
exemple, pour une cache de $4$ mots par blocs et une DRAM de $2$ mots par
blocs, la cache vas s'occuper de populer ses $4$ mots en fesant deux appel
à la DRAM. Si une instruction lui demande $8$ mots, la cache vas les fournir
dans le même temps qu'une instruction qui en demande qu'un seul.

Ceci simplifie grandement l'optimisation de la cache. Le nombre de mots optimal
est $2$ par blocs. Ça permet des remplacements de blocs sur des parties
partielles. Donc si un vecteur de $4$ mots à été mis en cache, $2$ blocs ont
été utilisés. Si la cache doit faire un remplacement de blocs pour une
nouvelle donnée, alors uniquement la moitiée du vecteur est remplacé. Donc
si il est réutilisé plus tard, on aura une pénalité d'accès à la DRAM au lieu
de $2$ car uniquement une partie est à aller chercher. Si la cache était $4$
mots, le vecteur au complet aurait été remplacé, nécessitant donc $2$ pénalités
pour allé le chercher dans la DRAM.

Pour le type de cache (direct, associatif par ensemble, ou associative),
l'utilisation d'ensemble semble pertinant. En effet les vecteurs réutilisé
pourrait être mis dans le premier ensemble et le reste dans le deuxième puisque
les accès aux matrices $4\times4$ est fait qu'une fois et les données ne sont
pas réutilisé. Ceci réduirait la taille de la cache. Malheureusement, ceci
ajoute une complexitée. La cache se popule comme elle veut. Donc il faudrait
être sur que le code assembleur popule la cache dans le bon ordre afin que des
données de matrices $4\times4$ ne se trouve pas dans le premier ensemble.

Si c'est fait, alors une cache associatif par ensemble de $8$ blocs de $2$ mots
avec $2$ ensemble semble très optimisé pour le calcul de référence en prennant
pour aquis que l'ordre d'appel des données est optimisé pour la cache. Ce
serait beaucoup mieux qu'une cache directe de $32$ blocs de $2$ mots.

Hors une analyze du code de référence confirme l'ordre de chargement des
données. Les vecteurs d'entrées et sorties sont chargé en premier et donc
vont être mis dans le premier ensemble. La cache populera le deuxième avec
les données de matrices. Ceci est donc la meilleur optimisation pour le code de
référence. Elle optimise à la fois les accès échoué et la grosseur de la cache.

\subsection{Optimisation en fonction de Viterbis}
Viterbis utilise un index de boucle qui est enregistrer dans la mémoire afin de
ne pas causer problème avec les fonctions appellé à l'interne. L'optimisation
discuté plus haut ne prend pas en compte l'ordre d'appel du code Viterbis, ni
le nombre de données différentes.

\section{Intégration}

\subsection{Ébauche de priorisation en fonction du temps ressenti}
Ce qui prend le moins de temps, sont les modifications logicielles. Si un non
respect total des standard MIPS optimise drastiquement le code Viterbis ou le
code de référence, alors ce serait une priorité.

Par la suite vient les modifications physique. L'entreprise à déja quelque
chose qui permet de créé des caches modulable. L'ajout d'une cache
d'instruction est plus prioritaire que celle de données car elle présente un
immense gain de performance pour les cas d'étude. Ensuite viendrais la cache
de données.

Par la suite, le changement d'organisation en pipeline avec branchement au 4e
étage est moins dur que celle étudié à deux étages avec les modules logiques
supplémentaire. Chacune permet un gain de performances. Des architectures de
processeurs sont déjà disponible en ligne, surtout pour de simple processeurs
sans grande complexité comme ceux des cas d'études. Cependant ceci nécessite
une restructuration de processeur, ce qui peut être plus couteux que prévu.
C'est dur à dire.

Pour finir, l'implémentation des instructions SIMD est la plus couteuse.
Non seulement le physique doit être changé, mais un temps de dévelopment à
haut risque est ajouté. Certaines instructions peuvent n'être utilisé que pour
ce cas-ci, rendent la versatilité moins attrayante.

\subsection{Priorisation en fonction de gain}
Sachant que le total de coups d'horloge sans aucun changement, pour
l'organisation unicycle est le suivant:
\begin{align}
  \text{T} &= 10\times\text{ instructions } + 10\times\text{ accès données } + instructions \\
  \text{T} &= 3930 + 640 + 393 \\
  \text{T} &= 4963
\end{align}

Le plus gros gain est d'implémenter les instructions SIMD. En effet le nombres
d'instructions passe de $393$ à $36$. Ce changement impacte à la fois les
pénalités des caches d'instructions et les pénalités d'accès à la DRAM de
données. En effet, dans le nouveau program de référence, au lieu d'avoir $4$
instructions accèdant à la DRAM dans deux boucles éxecuté $4$ fois chaque,
uniquement $2$ impactant la mémoire de données sont fait et ce juste $4$ fois.
La nouvelle formule est donc:
\begin{align}
  \text{T} &= 10\times36 + 10\times(2\times4+1) + 36 \\
  \text{T} &= 360 + 90 + 36 \\
  \text{T} &= 486
\end{align}
On observe donc une amélioration par un facteur de 


Le plus gros gain est celui de la cache d'instructions. Son implémentation
réduit d'un facteur de $32.75$ les accès aux instructions, donc d'un facteur
de $4$ le nombre de coups d'horloge du système unicycle! Cette optimisation
réponderait directement au gain de $3$ demandé dans la problématique, mais
n'apporte aucune réduction du nombre d'instructions. C'est donc une priorité.

Changé l'organisation pour être en pipeline 2e étages serais la prochaine chose
à faire. En effet, la durée d'un coup d'horloge baisse de $25ns$ à $10ns$, mais
le nombre d'instructions total executé change. On passe d'environ $124$us à
$55$us. Un gain de $2.2$.

%\todo{proposer un système optimal et prioriser les modifications par leurs
%gains effectifs sur la performance et leur temps de développement}

\end{document}
